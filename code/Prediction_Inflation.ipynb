{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "class EnhancedInflationPredictor:\n",
    "    def __init__(self, lookback=4, lstm_units=32, epochs=300, n_models=5):\n",
    "        self.lookback = lookback\n",
    "        self.lstm_units = lstm_units\n",
    "        self.epochs = epochs\n",
    "        self.n_models = n_models\n",
    "        self.scalers = {}\n",
    "        self.imputer = KNNImputer(n_neighbors=3)\n",
    "        self.models = {}\n",
    "        \n",
    "    def prepare_data(self, data, target_col='FP_CPI_TOTL_ZG'):\n",
    "        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        if target_col not in numeric_cols:\n",
    "            raise ValueError(f\"Target column {target_col} not found\")\n",
    "        \n",
    "        if numeric_cols[-1] != target_col:\n",
    "            numeric_cols.remove(target_col)\n",
    "            numeric_cols.append(target_col)\n",
    "        \n",
    "        data_clean = data[numeric_cols].copy()\n",
    "        \n",
    "        data_imputed = self.imputer.fit_transform(data_clean)\n",
    "        data_clean = pd.DataFrame(data_imputed, \n",
    "                                 index=data.index, \n",
    "                                 columns=data_clean.columns)\n",
    "        \n",
    "        return data_clean\n",
    "    \n",
    "    def create_dataset(self, data, target_col='FP_CPI_TOTL_ZG'):\n",
    "        X, y = [], []\n",
    "        data_values = data.values\n",
    "        \n",
    "        target_idx = data.columns.get_loc(target_col)\n",
    "        \n",
    "        for i in range(self.lookback, len(data_values)):\n",
    "            X.append(data_values[i-self.lookback:i, :])\n",
    "            y.append(data_values[i, target_idx])\n",
    "            \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def build_model(self, input_shape):\n",
    "        if hasattr(tf.keras.optimizers, 'legacy'):\n",
    "            optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "        else:\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        model = Sequential([\n",
    "            LSTM(self.lstm_units, \n",
    "                 activation='tanh',\n",
    "                 input_shape=input_shape,\n",
    "                 return_sequences=True,\n",
    "                 kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.3),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            LSTM(self.lstm_units//2,\n",
    "                 activation='tanh',\n",
    "                 kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.3),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_region_model(self, region_data, region_name):\n",
    "        prepared_data = self.prepare_data(region_data)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(prepared_data)\n",
    "        self.scalers[region_name] = scaler\n",
    "        \n",
    "        X, y = self.create_dataset(\n",
    "            pd.DataFrame(scaled_data, columns=prepared_data.columns)\n",
    "        )\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            raise ValueError(f\"Insufficient data for training\")\n",
    "        \n",
    "        models = []\n",
    "        \n",
    "        for i in range(self.n_models):\n",
    "            model = self.build_model((self.lookback, X.shape[2]))\n",
    "            \n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss', patience=50, restore_best_weights=True\n",
    "            )\n",
    "            \n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.5, patience=20\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X, y,\n",
    "                epochs=self.epochs,\n",
    "                batch_size=8,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[early_stopping, reduce_lr],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            models.append(model)\n",
    "        \n",
    "        self.models[region_name] = {\n",
    "            'models': models,\n",
    "            'feature_names': prepared_data.columns.tolist(),\n",
    "            'last_sequence': scaled_data[-self.lookback:]\n",
    "        }\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def predict_future(self, region_name, years=10):\n",
    "        if region_name not in self.models:\n",
    "            raise ValueError(f\"No trained model found for {region_name}\")\n",
    "        \n",
    "        model_info = self.models[region_name]\n",
    "        scaler = self.scalers[region_name]\n",
    "        models = model_info['models']\n",
    "        \n",
    "        predictions = []\n",
    "        current_sequence = model_info['last_sequence'].copy()\n",
    "        \n",
    "        for year in range(years):\n",
    "            year_predictions = []\n",
    "            \n",
    "            for model in models:\n",
    "                input_seq = current_sequence.reshape(1, self.lookback, -1)\n",
    "                pred_scaled = model.predict(input_seq, verbose=0)[0, 0]\n",
    "                year_predictions.append(pred_scaled)\n",
    "            \n",
    "            ensemble_pred = np.mean(year_predictions)\n",
    "            \n",
    "            new_point = current_sequence[-1].copy()\n",
    "            new_point[-1] = ensemble_pred\n",
    "            \n",
    "            current_sequence = np.roll(current_sequence, -1, axis=0)\n",
    "            current_sequence[-1] = new_point\n",
    "            \n",
    "            dummy_array = np.zeros((1, len(model_info['feature_names'])))\n",
    "            dummy_array[0, -1] = ensemble_pred\n",
    "            pred_original = scaler.inverse_transform(dummy_array)[0, -1]\n",
    "            \n",
    "            predictions.append(pred_original)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "def main():\n",
    "    input_file = \"/Users/oushilin/Desktop/AAAI/Data/comprehensive_economic_data.xlsx\"\n",
    "    output_file = \"/Users/oushilin/Desktop/AAAI/Data/inflation_forecasts_2026_2035.xlsx\"\n",
    "    \n",
    "    china_data = pd.read_excel(input_file, sheet_name='China', index_col='year')\n",
    "    eu_data = pd.read_excel(input_file, sheet_name='European Union', index_col='year')\n",
    "    mena_data = pd.read_excel(input_file, sheet_name='Middle East & North Africa', index_col='year')\n",
    "    \n",
    "    regions_data = {\n",
    "        'China': china_data,\n",
    "        'European Union': eu_data,\n",
    "        'Middle East & North Africa': mena_data\n",
    "    }\n",
    "    \n",
    "    predictor = EnhancedInflationPredictor(\n",
    "        lookback=4,\n",
    "        lstm_units=32,\n",
    "        epochs=300,\n",
    "        n_models=5\n",
    "    )\n",
    "    \n",
    "    forecasts = {}\n",
    "    regions = ['China', 'European Union', 'Middle East & North Africa']\n",
    "    \n",
    "    for region in regions:\n",
    "        predictor.train_region_model(regions_data[region], region)\n",
    "        future_predictions = predictor.predict_future(region, years=10)\n",
    "        forecasts[region] = future_predictions\n",
    "    \n",
    "    years = list(range(2026, 2036))\n",
    "    results_data = []\n",
    "    \n",
    "    for i, year in enumerate(years):\n",
    "        row = {'Year': year}\n",
    "        for region in regions:\n",
    "            row[region] = forecasts[region][i]\n",
    "        results_data.append(row)\n",
    "    \n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    results_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    print(\"Inflation Forecasts 2026-2035:\")\n",
    "    print(\"=\" * 50)\n",
    "    for region in regions:\n",
    "        print(f\"\\n{region}:\")\n",
    "        for i, year in enumerate(years):\n",
    "            print(f\"  {year}: {forecasts[region][i]:.2f}%\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
