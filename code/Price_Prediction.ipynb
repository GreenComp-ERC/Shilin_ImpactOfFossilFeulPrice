{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading and cleaning crude oil price data...\n",
      "Data loaded successfully. Shape after cleaning: (779, 5)\n",
      "\n",
      "2. Creating advanced features (including cyclical features)...\n",
      "New features created:\n",
      "            Crude_oil_average  Year  Month_Sin     Month_Cos  Rolling_Avg_3M  \\\n",
      "Date                                                                           \n",
      "1960-01-01               1.63  1960   0.500000  8.660254e-01            1.63   \n",
      "1960-02-01               1.63  1960   0.866025  5.000000e-01            1.63   \n",
      "1960-03-01               1.63  1960   1.000000  6.123234e-17            1.63   \n",
      "1960-04-01               1.63  1960   0.866025 -5.000000e-01            1.63   \n",
      "1960-05-01               1.63  1960   0.500000 -8.660254e-01            1.63   \n",
      "\n",
      "            Rolling_Avg_12M  \n",
      "Date                         \n",
      "1960-01-01             1.63  \n",
      "1960-02-01             1.63  \n",
      "1960-03-01             1.63  \n",
      "1960-04-01             1.63  \n",
      "1960-05-01             1.63  \n",
      "\n",
      "3. Performing multivariate data preprocessing...\n",
      "Training data shape: X_full=(767, 12, 6), y_full=(767,)\n",
      "\n",
      "4. Building the tuned deep learning model...\n",
      "\n",
      "5. Training the tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)        [(None, 12, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 12, 80)               1520      ['input_7[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 12, 80)               0         ['conv1d_6[0][0]']            \n",
      "                                                                                                  \n",
      " bidirectional_6 (Bidirecti  (None, 12, 256)              214016    ['dropout_14[0][0]']          \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 12, 256)              0         ['bidirectional_6[0][0]']     \n",
      "                                                                                                  \n",
      " gru_2 (GRU)                 (None, 12, 64)               61824     ['dropout_15[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)        (None, 12, 64)               0         ['gru_2[0][0]']               \n",
      "                                                                                                  \n",
      " attention_6 (Attention)     (None, 12, 64)               0         ['dropout_16[0][0]',          \n",
      "                                                                     'dropout_16[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate  (None, 12, 128)              0         ['dropout_16[0][0]',          \n",
      " )                                                                   'attention_6[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 12, 128)              256       ['concatenate_6[0][0]']       \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling1d_6  (None, 128)                  0         ['layer_normalization_6[0][0]'\n",
      "  (GlobalAveragePooling1D)                                          ]                             \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 80)                   10320     ['global_average_pooling1d_6[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 1)                    81        ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 288017 (1.10 MB)\n",
      "Trainable params: 288017 (1.10 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1759273719.016578       1 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 2s 19ms/step - loss: 4.5679 - lr: 5.0000e-04\n",
      "Epoch 2/200\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 3.4865 - lr: 5.0000e-04\n",
      "Epoch 3/200\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 2.7067 - lr: 5.0000e-04\n",
      "Epoch 4/200\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 2.1221 - lr: 5.0000e-04\n",
      "Epoch 5/200\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 1.6874 - lr: 5.0000e-04\n",
      "Epoch 6/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 1.3642 - lr: 5.0000e-04\n",
      "Epoch 7/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 1.1232 - lr: 5.0000e-04\n",
      "Epoch 8/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.9420 - lr: 5.0000e-04\n",
      "Epoch 9/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.8035 - lr: 5.0000e-04\n",
      "Epoch 10/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.6970 - lr: 5.0000e-04\n",
      "Epoch 11/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.6128 - lr: 5.0000e-04\n",
      "Epoch 12/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.5452 - lr: 5.0000e-04\n",
      "Epoch 13/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.4890 - lr: 5.0000e-04\n",
      "Epoch 14/200\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 0.4412 - lr: 5.0000e-04\n",
      "Epoch 15/200\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 0.4010 - lr: 5.0000e-04\n",
      "Epoch 16/200\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 0.3649 - lr: 5.0000e-04\n",
      "Epoch 17/200\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 0.3337 - lr: 5.0000e-04\n",
      "Epoch 18/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.3048 - lr: 5.0000e-04\n",
      "Epoch 19/200\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 0.2796 - lr: 5.0000e-04\n",
      "Epoch 20/200\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 0.2567 - lr: 5.0000e-04\n",
      "Epoch 21/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.2356 - lr: 5.0000e-04\n",
      "Epoch 22/200\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 0.2164 - lr: 5.0000e-04\n",
      "Epoch 23/200\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 0.1989 - lr: 5.0000e-04\n",
      "Epoch 24/200\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 0.1824 - lr: 5.0000e-04\n",
      "Epoch 25/200\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 0.1682 - lr: 5.0000e-04\n",
      "Epoch 26/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.1555 - lr: 5.0000e-04\n",
      "Epoch 27/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.1431 - lr: 5.0000e-04\n",
      "Epoch 28/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.1323 - lr: 5.0000e-04\n",
      "Epoch 29/200\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 0.1219 - lr: 5.0000e-04\n",
      "Epoch 30/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.1119 - lr: 5.0000e-04\n",
      "Epoch 31/200\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 0.1040 - lr: 5.0000e-04\n",
      "Epoch 32/200\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 0.0959 - lr: 5.0000e-04\n",
      "Epoch 33/200\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 0.0892 - lr: 5.0000e-04\n",
      "Epoch 34/200\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 0.0821 - lr: 5.0000e-04\n",
      "Epoch 35/200\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 0.0762 - lr: 5.0000e-04\n",
      "Epoch 36/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.0705 - lr: 5.0000e-04\n",
      "Epoch 37/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0657 - lr: 5.0000e-04\n",
      "Epoch 38/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.0610 - lr: 5.0000e-04\n",
      "Epoch 39/200\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 0.0564 - lr: 5.0000e-04\n",
      "Epoch 40/200\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 0.0530 - lr: 5.0000e-04\n",
      "Epoch 41/200\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 0.0491 - lr: 5.0000e-04\n",
      "Epoch 42/200\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 0.0454 - lr: 5.0000e-04\n",
      "Epoch 43/200\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 0.0416 - lr: 5.0000e-04\n",
      "Epoch 44/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.0396 - lr: 5.0000e-04\n",
      "Epoch 45/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0359 - lr: 5.0000e-04\n",
      "Epoch 46/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.0340 - lr: 5.0000e-04\n",
      "Epoch 47/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0331 - lr: 5.0000e-04\n",
      "Epoch 48/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.0293 - lr: 5.0000e-04\n",
      "Epoch 49/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.0276 - lr: 5.0000e-04\n",
      "Epoch 50/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0254 - lr: 5.0000e-04\n",
      "Epoch 51/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0240 - lr: 5.0000e-04\n",
      "Epoch 52/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0227 - lr: 5.0000e-04\n",
      "Epoch 53/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.0208 - lr: 5.0000e-04\n",
      "Epoch 54/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0199 - lr: 5.0000e-04\n",
      "Epoch 55/200\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 0.0185 - lr: 5.0000e-04\n",
      "Epoch 56/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.0172 - lr: 5.0000e-04\n",
      "Epoch 57/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0159 - lr: 5.0000e-04\n",
      "Epoch 58/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.0160 - lr: 5.0000e-04\n",
      "Epoch 59/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0159 - lr: 5.0000e-04\n",
      "Epoch 60/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0141 - lr: 5.0000e-04\n",
      "Epoch 61/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0127 - lr: 5.0000e-04\n",
      "Epoch 62/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0122 - lr: 5.0000e-04\n",
      "Epoch 63/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0121 - lr: 5.0000e-04\n",
      "Epoch 64/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0113 - lr: 5.0000e-04\n",
      "Epoch 65/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0118 - lr: 5.0000e-04\n",
      "Epoch 66/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0102 - lr: 5.0000e-04\n",
      "Epoch 67/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0107 - lr: 5.0000e-04\n",
      "Epoch 68/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0098 - lr: 5.0000e-04\n",
      "Epoch 69/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0095 - lr: 5.0000e-04\n",
      "Epoch 70/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0087 - lr: 5.0000e-04\n",
      "Epoch 71/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.0085 - lr: 5.0000e-04\n",
      "Epoch 72/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0085 - lr: 5.0000e-04\n",
      "Epoch 73/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0077 - lr: 5.0000e-04\n",
      "Epoch 74/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0075 - lr: 5.0000e-04\n",
      "Epoch 75/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0074 - lr: 5.0000e-04\n",
      "Epoch 76/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0076 - lr: 5.0000e-04\n",
      "Epoch 77/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0079 - lr: 5.0000e-04\n",
      "Epoch 78/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0068 - lr: 5.0000e-04\n",
      "Epoch 79/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0067 - lr: 5.0000e-04\n",
      "Epoch 80/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0064 - lr: 5.0000e-04\n",
      "Epoch 81/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0067 - lr: 5.0000e-04\n",
      "Epoch 82/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0062 - lr: 5.0000e-04\n",
      "Epoch 83/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0070 - lr: 5.0000e-04\n",
      "Epoch 84/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0056 - lr: 5.0000e-04\n",
      "Epoch 85/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0052 - lr: 5.0000e-04\n",
      "Epoch 86/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0052 - lr: 5.0000e-04\n",
      "Epoch 87/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0063 - lr: 5.0000e-04\n",
      "Epoch 88/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0055 - lr: 5.0000e-04\n",
      "Epoch 89/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0047 - lr: 5.0000e-04\n",
      "Epoch 90/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0050 - lr: 5.0000e-04\n",
      "Epoch 91/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0051 - lr: 5.0000e-04\n",
      "Epoch 92/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0050 - lr: 5.0000e-04\n",
      "Epoch 93/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0045 - lr: 5.0000e-04\n",
      "Epoch 94/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0052 - lr: 5.0000e-04\n",
      "Epoch 95/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0048 - lr: 5.0000e-04\n",
      "Epoch 96/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0043 - lr: 5.0000e-04\n",
      "Epoch 97/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0052 - lr: 5.0000e-04\n",
      "Epoch 98/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0050 - lr: 5.0000e-04\n",
      "Epoch 99/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0054 - lr: 5.0000e-04\n",
      "Epoch 100/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0049 - lr: 5.0000e-04\n",
      "Epoch 101/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0042 - lr: 5.0000e-04\n",
      "Epoch 102/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0042 - lr: 5.0000e-04\n",
      "Epoch 103/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.0044 - lr: 5.0000e-04\n",
      "Epoch 104/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.0043 - lr: 5.0000e-04\n",
      "Epoch 105/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0039 - lr: 5.0000e-04\n",
      "Epoch 106/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0041 - lr: 5.0000e-04\n",
      "Epoch 107/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.0036 - lr: 5.0000e-04\n",
      "Epoch 108/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0044 - lr: 5.0000e-04\n",
      "Epoch 109/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0039 - lr: 5.0000e-04\n",
      "Epoch 110/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0048 - lr: 5.0000e-04\n",
      "Epoch 111/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0059 - lr: 5.0000e-04\n",
      "Epoch 112/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0046 - lr: 5.0000e-04\n",
      "Epoch 113/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0042 - lr: 5.0000e-04\n",
      "Epoch 114/200\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.0042 - lr: 5.0000e-04\n",
      "Epoch 115/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0043 - lr: 5.0000e-04\n",
      "Epoch 116/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0038 - lr: 5.0000e-04\n",
      "Epoch 117/200\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.0036 - lr: 5.0000e-04\n",
      "Epoch 118/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0031 - lr: 1.0000e-04\n",
      "Epoch 119/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0030 - lr: 1.0000e-04\n",
      "Epoch 120/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0031 - lr: 1.0000e-04\n",
      "Epoch 121/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0031 - lr: 1.0000e-04\n",
      "Epoch 122/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0032 - lr: 1.0000e-04\n",
      "Epoch 123/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.0031 - lr: 1.0000e-04\n",
      "Epoch 124/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0034 - lr: 1.0000e-04\n",
      "Epoch 125/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0031 - lr: 1.0000e-04\n",
      "Epoch 126/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0035 - lr: 1.0000e-04\n",
      "Epoch 127/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0032 - lr: 1.0000e-04\n",
      "Epoch 128/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0032 - lr: 1.0000e-04\n",
      "Epoch 129/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0029 - lr: 2.0000e-05\n",
      "Epoch 130/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0030 - lr: 2.0000e-05\n",
      "Epoch 131/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0030 - lr: 2.0000e-05\n",
      "Epoch 132/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0029 - lr: 2.0000e-05\n",
      "Epoch 133/200\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.0029 - lr: 2.0000e-05\n",
      "Epoch 134/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0028 - lr: 2.0000e-05\n",
      "Epoch 135/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0032 - lr: 2.0000e-05\n",
      "Epoch 136/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0030 - lr: 2.0000e-05\n",
      "Epoch 137/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0032 - lr: 2.0000e-05\n",
      "Epoch 138/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0032 - lr: 2.0000e-05\n",
      "Epoch 139/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0031 - lr: 2.0000e-05\n",
      "Epoch 140/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0030 - lr: 4.0000e-06\n",
      "Epoch 141/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0029 - lr: 4.0000e-06\n",
      "Epoch 142/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0031 - lr: 4.0000e-06\n",
      "Epoch 143/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0031 - lr: 4.0000e-06\n",
      "Epoch 144/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0030 - lr: 4.0000e-06\n",
      "Epoch 145/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0032 - lr: 4.0000e-06\n",
      "Epoch 146/200\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.0028 - lr: 4.0000e-06\n",
      "Epoch 147/200\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.0031 - lr: 4.0000e-06\n",
      "Epoch 148/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0030 - lr: 4.0000e-06\n",
      "Epoch 149/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0031 - lr: 4.0000e-06\n",
      "Epoch 150/200\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.0032 - lr: 8.0000e-07\n",
      "Epoch 151/200\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.0029 - lr: 8.0000e-07\n",
      "Epoch 152/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0029 - lr: 8.0000e-07\n",
      "Epoch 153/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0029 - lr: 8.0000e-07\n",
      "Epoch 154/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0031 - lr: 8.0000e-07\n",
      "\n",
      "6. Performing iterative forecasting...\n",
      "Forecasting 133 months from 2024-12 to 2035-12.\n",
      "Forecasting 2024-12...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1759273786.031938       1 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasting 2035-12...\n",
      "Initial forecast generated.\n",
      "\n",
      "7. Applying Gaussian Process Regression model...\n",
      "Gaussian Process Regression model processing complete.\n",
      "\n",
      "8. Saving final forecast to CSV file...\n",
      "\n",
      "Forecast successfully saved to '/Users/oushilin/Desktop/AAAI/Data/oil_price_forecast_to_2035.csv'\n",
      "\n",
      "--- Forecast Sample ---\n",
      "      Date  Predicted_Price  Confidence_Interval_Lower  \\\n",
      "0  2024-12        71.641331                  71.447615   \n",
      "1  2025-01        71.922300                  71.753342   \n",
      "2  2025-02        73.882032                  73.743624   \n",
      "3  2025-03        77.037975                  76.899594   \n",
      "4  2025-04        80.554433                  80.423773   \n",
      "\n",
      "   Confidence_Interval_Upper  \n",
      "0                  71.835047  \n",
      "1                  72.091259  \n",
      "2                  74.020440  \n",
      "3                  77.176356  \n",
      "4                  80.685092  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (LSTM, Dense, Dropout, Bidirectional, Attention,\n",
    "                                     Concatenate, Input, GlobalAveragePooling1D, GRU,\n",
    "                                     Conv1D, LayerNormalization)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Load and Clean Data (Adapted for Crude Oil) ---\n",
    "print(\"1. Loading and cleaning crude oil price data...\")\n",
    "try:\n",
    "    # Define column names for the crude oil data file\n",
    "    column_names = ['Date', 'Crude_oil_average', 'Crude_oil_Brent', 'Crude_oil_Dubai', 'Crude_oil_WTI']\n",
    "    \n",
    "    # Read the Excel file for crude oil (Sheet1)\n",
    "    data = pd.read_excel(\n",
    "        'Average_oil_price.xlsx', sheet_name='Sheet1', skiprows=3, header=None,\n",
    "        names=column_names, na_values=['…', '...', '–', '-', 'nan', '']\n",
    "    )\n",
    "    \n",
    "    # Parse the date format (e.g., '1960M01')\n",
    "    data['Date'] = pd.to_datetime(data['Date'].str.replace('M', '-'), errors='coerce')\n",
    "    \n",
    "    # Set the target column to 'Crude_oil_average'\n",
    "    data['Crude_oil_average'] = pd.to_numeric(data['Crude_oil_average'], errors='coerce')\n",
    "    data.dropna(subset=['Date', 'Crude_oil_average'], inplace=True)\n",
    "    data = data.sort_values('Date').reset_index(drop=True)\n",
    "    print(f\"Data loaded successfully. Shape after cleaning: {data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'Average_oil_price.xlsx' not found. Please ensure the file is in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Feature Engineering (with Cyclical Features) ---\n",
    "print(\"\\n2. Creating advanced features (including cyclical features)...\")\n",
    "df = data[['Date', 'Crude_oil_average']].copy()\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Basic time features\n",
    "df['Year'] = df.index.year\n",
    "month = df.index.month\n",
    "\n",
    "# New: Convert month to cyclical features\n",
    "df['Month_Sin'] = np.sin(2 * np.pi * month / 12)\n",
    "df['Month_Cos'] = np.cos(2 * np.pi * month / 12)\n",
    "\n",
    "# Trend features\n",
    "df['Rolling_Avg_3M'] = df['Crude_oil_average'].rolling(window=3).mean()\n",
    "df['Rolling_Avg_12M'] = df['Crude_oil_average'].rolling(window=12).mean()\n",
    "\n",
    "df.bfill(inplace=True)\n",
    "df.ffill(inplace=True)\n",
    "print(\"New features created:\")\n",
    "print(df.head())\n",
    "\n",
    "# --- 3. Data Preprocessing ---\n",
    "print(\"\\n3. Performing multivariate data preprocessing...\")\n",
    "# Update feature columns\n",
    "feature_columns = ['Crude_oil_average', 'Year', 'Month_Sin', 'Month_Cos', 'Rolling_Avg_3M', 'Rolling_Avg_12M']\n",
    "target_column = 'Crude_oil_average'\n",
    "\n",
    "features = df[feature_columns].values\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(features)\n",
    "\n",
    "def create_multivariate_dataset(data, target_col_index, time_step=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data[i:(i+time_step), :])\n",
    "        y.append(data[i + time_step, target_col_index])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 12\n",
    "target_col_index = feature_columns.index(target_column)\n",
    "X_full, y_full = create_multivariate_dataset(scaled_data, target_col_index, time_step)\n",
    "num_features = X_full.shape[2]\n",
    "print(f\"Training data shape: X_full={X_full.shape}, y_full={y_full.shape}\")\n",
    "\n",
    "# --- 4. Build Tuned Model ---\n",
    "print(\"\\n4. Building the tuned deep learning model...\")\n",
    "def build_tuned_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    conv_layer = Conv1D(80, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
    "    dropout_layer_0 = Dropout(0.3)(conv_layer)\n",
    "    \n",
    "    lstm_layer = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer='l2'))(dropout_layer_0)\n",
    "    dropout_layer_1 = Dropout(0.5)(lstm_layer)\n",
    "    \n",
    "    gru_layer = GRU(64, return_sequences=True, kernel_regularizer='l2')(dropout_layer_1)\n",
    "    dropout_layer_2 = Dropout(0.5)(gru_layer)\n",
    "\n",
    "    attention_layer = Attention()([dropout_layer_2, dropout_layer_2])\n",
    "    attention_layer = Concatenate(axis=-1)([dropout_layer_2, attention_layer])\n",
    "    \n",
    "    norm_layer = LayerNormalization()(attention_layer)\n",
    "    pooled_layer = GlobalAveragePooling1D()(norm_layer)\n",
    "    \n",
    "    dense_layer = Dense(80, activation='relu')(pooled_layer)\n",
    "    output_layer = Dense(1)(dense_layer)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# --- 5. Train Model ---\n",
    "print(\"\\n5. Training the tuned model...\")\n",
    "model = build_tuned_model(input_shape=(time_step, num_features))\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=10, min_lr=1e-7)\n",
    "model.fit(X_full, y_full, epochs=200, batch_size=32, callbacks=[early_stopping, lr_scheduler], verbose=1)\n",
    "\n",
    "# --- 6. Iterative Forecasting (with Dynamic Period Calculation) ---\n",
    "print(\"\\n6. Performing iterative forecasting...\")\n",
    "# Dynamically calculate the number of steps to forecast until Dec 2035\n",
    "last_date = df.index[-1]\n",
    "start_date = last_date + pd.DateOffset(months=1)\n",
    "end_date = pd.to_datetime('2035-12-01')\n",
    "num_forecast_steps = (end_date.year - start_date.year) * 12 + (end_date.month - start_date.month) + 1\n",
    "print(f\"Forecasting {num_forecast_steps} months from {start_date.strftime('%Y-%m')} to {end_date.strftime('%Y-%m')}.\")\n",
    "\n",
    "future_dates = pd.date_range(start=start_date, periods=num_forecast_steps, freq='MS')\n",
    "last_sequence_scaled = scaled_data[-time_step:].tolist()\n",
    "historical_prices = df[target_column].tolist()\n",
    "future_predictions = []\n",
    "\n",
    "for date in future_dates:\n",
    "    print(f\"Forecasting {date.strftime('%Y-%m')}...\", end=\"\\r\")\n",
    "    current_input = np.array([last_sequence_scaled])\n",
    "    next_pred_scaled = model.predict(current_input, verbose=0)[0, 0]\n",
    "    \n",
    "    dummy_array = np.zeros((1, num_features))\n",
    "    dummy_array[0, target_col_index] = next_pred_scaled\n",
    "    next_pred_real = scaler.inverse_transform(dummy_array)[0, target_col_index]\n",
    "    \n",
    "    future_predictions.append(next_pred_real)\n",
    "    historical_prices.append(next_pred_real)\n",
    "    \n",
    "    # Dynamically create features for the next time step, including cyclical ones\n",
    "    next_year = date.year\n",
    "    next_month = date.month\n",
    "    next_month_sin = np.sin(2 * np.pi * next_month / 12)\n",
    "    next_month_cos = np.cos(2 * np.pi * next_month / 12)\n",
    "    next_roll_3m = pd.Series(historical_prices).rolling(window=3).mean().iloc[-1]\n",
    "    next_roll_12m = pd.Series(historical_prices).rolling(window=12).mean().iloc[-1]\n",
    "    \n",
    "    next_features_real = np.array([[next_pred_real, next_year, next_month_sin, next_month_cos, next_roll_3m, next_roll_12m]])\n",
    "    next_features_scaled = scaler.transform(next_features_real)[0]\n",
    "    \n",
    "    last_sequence_scaled.pop(0)\n",
    "    last_sequence_scaled.append(next_features_scaled.tolist())\n",
    "\n",
    "print(\"\\nInitial forecast generated.\")\n",
    "initial_predictions = np.array(future_predictions)\n",
    "\n",
    "# --- 7. Apply GPR for Smoothing and Confidence Intervals ---\n",
    "print(\"\\n7. Applying Gaussian Process Regression model...\")\n",
    "future_dates_numeric = np.array(range(len(future_dates))).reshape(-1, 1)\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2)) + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-2, random_state=42)\n",
    "gpr.fit(future_dates_numeric, initial_predictions)\n",
    "gpr_predictions, gpr_std = gpr.predict(future_dates_numeric, return_std=True)\n",
    "ci_upper = gpr_predictions + 1.96 * gpr_std\n",
    "ci_lower = gpr_predictions - 1.96 * gpr_std\n",
    "print(\"Gaussian Process Regression model processing complete.\")\n",
    "\n",
    "# --- 8. Save Final Results to CSV ---\n",
    "print(\"\\n8. Saving final forecast to CSV file...\")\n",
    "forecast_df = pd.DataFrame({\n",
    "    'Date': future_dates,\n",
    "    'Predicted_Price': gpr_predictions,\n",
    "    'Confidence_Interval_Lower': ci_lower,\n",
    "    'Confidence_Interval_Upper': ci_upper\n",
    "})\n",
    "forecast_df['Date'] = forecast_df['Date'].dt.strftime('%Y-%m')\n",
    "output_filename = 'oil_price_forecast_to_2035.csv'\n",
    "forecast_df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nForecast successfully saved to '{output_filename}'\")\n",
    "print(\"\\n--- Forecast Sample ---\")\n",
    "print(forecast_df.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
